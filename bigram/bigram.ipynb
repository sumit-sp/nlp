{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f52fa4e-af49-4e24-bc05-c47ddb59a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba2b196-9630-4f06-868f-1456408909da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a39e1a79-9ff7-4f83-88f2-7c24c89021f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of Dorothy and the Wizard in Oz\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no rest\n",
      "\n",
      "Number of unique chars: 82\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/wizard_of_oz.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:200])\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(f\"\\nNumber of unique chars: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1e5b0e-be4a-41a1-b503-7ba7eca64eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {c:i for i, c in enumerate(chars)}\n",
    "int_to_char = {i:c for i, c in enumerate(chars)}\n",
    "encoder = lambda s: [char_to_int[char] for char in s]\n",
    "decoder = lambda i: \"\".join([int_to_char[n] for n in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db5b3ff3-b3ac-4d57-8590-965a92152015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 59, 66, 66, 69, 1, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(\"hello !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "089fd92f-dc0d-4eeb-bc2d-2d3ef07ede2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello !'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder([62, 59, 66, 66, 69, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0efb6f1-4b93-4a5d-b2aa-92dfe1332422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoder(\"This is the Text !\"), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "008aa825-03ae-43ae-b921-fac53461c520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([45, 62, 63, 73,  1, 63, 73,  1, 74, 62, 59,  1, 45, 59, 78, 74,  1,  2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "308d9443-9dd5-49dd-b9c8-cd47e955a045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', 'doing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "# Example usage:\n",
    "text = \"Hello! How are you doing?\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5e6a259-0faf-4400-a27a-858686be502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram sequences: [(('the', 'quick', 'brown'), 'fox'), (('quick', 'brown', 'fox'), 'jumps'), (('brown', 'fox', 'jumps'), 'over'), (('fox', 'jumps', 'over'), 'the'), (('jumps', 'over', 'the'), 'lazy'), (('over', 'the', 'lazy'), 'dog')]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_sequences(words, n):\n",
    "    sequences = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        context = tuple(words[i:i+n-1])\n",
    "        next_word = words[i+n-1]\n",
    "        sequences.append((context, next_word))\n",
    "    return sequences\n",
    "\n",
    "# Example usage:\n",
    "processed_text = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "n = 4\n",
    "ngram_sequences = generate_ngram_sequences(processed_text, n)\n",
    "print(f\"{n}-gram sequences:\", ngram_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35bb71b7-ddda-4614-9d38-d89253c21087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.seed()\n",
    "torch.randint(5, (4, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e0a36df-9694-4da0-8528-571dfb1f734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Bigram Sequences:\n",
      "Context: the, Next Word: quick\n",
      "Context: quick, Next Word: brown\n",
      "Context: brown, Next Word: fox\n",
      "Context: fox, Next Word: jumps\n",
      "Context: jumps, Next Word: over\n",
      "Context: over, Next Word: the\n",
      "Context: the, Next Word: lazy\n",
      "Context: lazy, Next Word: dog\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Gather Text Data\n",
    "text_data = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Step 2: Preprocess Text Data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \"\")  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "processed_text = preprocess_text(text_data)\n",
    "\n",
    "# Step 3: Convert Text to Numerical Representation\n",
    "def tokenize_words(text):\n",
    "    # tokens = text.split()\n",
    "    tokens = sorted(set(text))\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "word_to_id, id_to_word = tokenize_words(processed_text)\n",
    "\n",
    "# Step 4: Generate Input-Output Pairs\n",
    "def generate_bigram_sequences(tokens):\n",
    "    sequences = []\n",
    "    for i in range(len(tokens) - 1):\n",
    "        context = tokens[i]\n",
    "        next_word = tokens[i + 1]\n",
    "        sequences.append((context, next_word))\n",
    "    return sequences\n",
    "\n",
    "sequences = generate_bigram_sequences(processed_text.split())\n",
    "\n",
    "# Print the generated sequences\n",
    "print(\"Generated Bigram Sequences:\")\n",
    "for context, next_word in sequences:\n",
    "    print(f\"Context: {context}, Next Word: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b105e466-67a6-4099-be9c-e1c76f39de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Inputs: tensor([4, 5, 1, 3])\n",
      "Labels: tensor([2, 7, 3, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a custom dataset class\n",
    "class BigramDataset(Dataset):\n",
    "    def __init__(self, sequences, word_to_id):\n",
    "        self.sequences = sequences\n",
    "        self.word_to_id = word_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, next_word = self.sequences[idx]\n",
    "        context_id = self.word_to_id[context]\n",
    "        next_word_id = self.word_to_id[next_word]\n",
    "        return torch.tensor(context_id), torch.tensor(next_word_id)\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = BigramDataset(sequences, word_to_id)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print the first batch of data\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Labels:\", labels)\n",
    "    print()\n",
    "    if batch_idx == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6336e318-52da-4766-90fb-d178bd99cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 5)  # Fully connected layer with input size 4 and output size 5\n",
    "        self.fc2 = nn.Linear(5, 4)   # Fully connected layer with input size 5 and output size 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation function to the output of the first layer\n",
    "        x = self.fc2(x)              # Pass the output through the second layer\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = SimpleNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df3802-28c5-4d72-8aa2-4645f82b3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 4\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Choose optimization algorithm and set hyperparameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for inputs, labels in test_loader:\n",
    "#         outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Accuracy on test set: {100 * accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5739f4a1-0232-4f39-a31d-61912b5d47a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0456,  0.2101, -0.3432, -0.4100,  0.2428], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28796562-fef3-4581-a6b3-9862be3fec8e",
   "metadata": {},
   "source": [
    "#### Each word/subword or more correctly token in the vocabulary will have it's own embedding vector of n-dimensions\n",
    "#### The vocabulary size is typically decided based on the size and diversity of the text corpus used for training.\n",
    "#### Hence, before start of the training we will have a large matrix of size (vocab_size, embeddings) from where each embedding is picked based on it's occurance during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f0d5c-1e97-4771-a4cb-7ff34fcbcf67",
   "metadata": {},
   "source": [
    "##### nn.Embedding is a PyTorch module that is used to create word embeddings in neural network models for natural language processing (NLP) tasks. It maps discrete tokens (e.g., words or characters) to continuous vectors of fixed size, allowing the model to learn representations of words in a dense embedding space.\n",
    "\n",
    "#### How nn.Embedding Works:\n",
    "\n",
    "##### nn.Embedding initializes a learnable embedding matrix where each row corresponds to the embedding vector of a token in the vocabulary. During forward pass, nn.Embedding takes an input tensor containing token indices and returns the corresponding embedding vectors. The embedding vectors are looked up from the embedding matrix based on the input token indices. The embedding vectors are then fed into the neural network model for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe8cf9-e760-4a70-a98c-6f40fb78828e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
